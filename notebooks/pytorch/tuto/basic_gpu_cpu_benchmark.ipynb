{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9737e286",
   "metadata": {},
   "source": [
    "# TUTORIAL : PyTorch basic computation using single CPU or GPU \n",
    "\n",
    "Tutorial adapted from [this PyTorch example](https://github.com/pytorch/examples/tree/master/mnist).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The aim of this tutorial is to use AI TRAINING product to train a simple model, on the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), with the PyTorch library and to compare performances of running it over CPU versus GPU.\n",
    "\n",
    "## Prerequities\n",
    "\n",
    "* a Public cloud project\n",
    "* an AI-TRAINING notebook job launched with the PyTorch preset image ([documentation available here](https://docs.ovh.com/gb/en/ai-training/start-use-notebooks/))\n",
    "* the notebook resources should have at least 1 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01fd54",
   "metadata": {},
   "source": [
    "### Step 1: Import PyTorch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea033de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a2e94",
   "metadata": {},
   "source": [
    "### Step 2: Check that you have GPU(s) available on your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5d36f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 (Tesla V100S-PCIE-32GB)\n",
      "cuda:1 (Tesla V100S-PCIE-32GB)\n"
     ]
    }
   ],
   "source": [
    "for device_index in range(torch.cuda.device_count()):\n",
    "    device = 'cuda:{}'.format(device_index)\n",
    "    device_name = torch.cuda.get_device_name(device)\n",
    "    print('{} ({})'.format(device, device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2135eca",
   "metadata": {},
   "source": [
    "### Step 3: Declare the neural network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c8c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68e4e7",
   "metadata": {},
   "source": [
    "### Step 4: Declare train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13bec402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, test_loader, lr=1.0, gamma=0.7):\n",
    "    print()\n",
    "    print('Train {}'.format(device))\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_one_epoch(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "def train_one_epoch(model, device, train_loader, optimizer, epoch):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return losses\n",
    "                \n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937f270",
   "metadata": {},
   "source": [
    "### Step 5: Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c067b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "dataset1 = datasets.MNIST('/workspace/data', train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST('/workspace/data', train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2495657e",
   "metadata": {},
   "source": [
    "### Step 5: Choose paramters to train our model\n",
    "\n",
    "We will train our model for only one epoch to make the benchmark run fast but you can increase this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6966b1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train cpu\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303818\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.228058\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.119387\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.184576\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.131525\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.124646\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.189758\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.110112\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.140365\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.042536\n",
      "\n",
      "Test set: Average loss: 0.0501, Accuracy: 9840/10000 (98%)\n",
      "\n",
      "\n",
      "Train cuda:0\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.294156\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.244684\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.176366\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.163390\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.124553\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.080185\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.137328\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.081987\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.192384\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.059394\n",
      "\n",
      "Test set: Average loss: 0.0576, Accuracy: 9802/10000 (98%)\n",
      "\n",
      "CPU took 55.28s\n",
      "GPU took 10.91s\n",
      "GPU is 5.1x times faster than CPU to train this model\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# Input batch size for training\n",
    "batch_size = 128\n",
    "# Input batch size for testing\n",
    "test_batch_size = 1000\n",
    "# Number of epochs to train\n",
    "epochs = 1\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=test_batch_size)\n",
    "\n",
    "# CPU benchmark\n",
    "device = 'cpu'\n",
    "model = Net().to(device)\n",
    "variables = {\n",
    "    'model': model, 'device': device, 'train_loader': train_loader, 'test_loader': test_loader,\n",
    "}\n",
    "cpu_time = timeit.timeit(f'train(model, device, train_loader, test_loader)', globals=variables, number=1, setup=\"from __main__ import train\")\n",
    "\n",
    "# GPU benchmark\n",
    "device = 'cuda:0'\n",
    "model = Net().to(device)\n",
    "variables = {\n",
    "    'model': model, 'device': device, 'train_loader': train_loader, 'test_loader': test_loader,\n",
    "}\n",
    "gpu_time = timeit.timeit(f'train(model, device, train_loader, test_loader)', globals=variables, number=1, setup=\"from __main__ import train\")\n",
    "\n",
    "# Results\n",
    "print('CPU took {:.2f}s'.format(cpu_time))\n",
    "print('GPU took {:.2f}s'.format(gpu_time))\n",
    "print('GPU is {:.1f}x times faster than CPU to train this model'.format(cpu_time / gpu_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb53f0",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This model train faster on GPU, it is small and the gap between CPU and GPU will be even greater with bigger models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309aec4",
   "metadata": {},
   "source": [
    "### Going further\n",
    "\n",
    "* For more information about running computations with PyTorch we advise you to follow the [official documentation](https://pytorch.org/docs/stable/index.html).\n",
    "* Resource consumption of your notebook is displayed in a dashboard that you can see. Just execute the following cells to get the URL corresponding to your notebook session. The credencials needed to access this dashboard are the same than those used for the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2cae534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your resource details URL is :\n",
      "https://monitoring.gra.training.ai.cloud.ovh.net/d/job/job-monitoring?orgId=1&from=now-5m&var-job=d2fe7e41-f6c7-48ec-a3d3-c0fe456167c0&to=now\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "JOB_ID = os.environ['JOB_ID']\n",
    "print(f'Your resource details URL is :')\n",
    "print(f'https://monitoring.gra.training.ai.cloud.ovh.net/d/job/job-monitoring?orgId=1&from=now-5m&var-job={JOB_ID}&to=now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c513ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
